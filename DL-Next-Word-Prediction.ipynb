{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c1ed6b",
   "metadata": {
    "id": "a7c1ed6b"
   },
   "source": [
    "# Objective: Next word prediction using RNN - LSTM\n",
    "\n",
    "##  If given the three words of a sentence, it should be able to guess the fourth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb68ce6",
   "metadata": {},
   "source": [
    "![](Image.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "p-TlDJIo93MW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-TlDJIo93MW",
    "outputId": "7638673c-1ab8-4f0a-808a-8a46866b8bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15.0\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
      "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.0\n",
      "    Uninstalling ml-dtypes-0.4.0:\n",
      "      Successfully uninstalled ml-dtypes-0.4.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.0\n",
      "    Uninstalling tensorboard-2.17.0:\n",
      "      Successfully uninstalled tensorboard-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961f084",
   "metadata": {
    "id": "7961f084"
   },
   "source": [
    "# 1.Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1af0c3",
   "metadata": {
    "id": "9c1af0c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d60d7",
   "metadata": {
    "id": "1d9d60d7"
   },
   "source": [
    "# 2. Open and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abb675a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "4abb675a",
    "outputId": "5133e6ff-c5f5-47d7-8b55-dc277125c455"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The Project Gutenberg eBook of Pride and Prejudice This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(r\"Pride and Prejudice.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "\n",
    "# replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces\n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79c674",
   "metadata": {
    "id": "cc79c674"
   },
   "source": [
    "# 3. Implement tokenization and make additional adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2416ca0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2416ca0",
    "outputId": "89b60144-2bbe-405a-94d7-228833cc8920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131180"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]\n",
    "len(sequence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50740646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50740646",
    "outputId": "0f03f19f-d94e-4e50-82f3-cad7b1ca5e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7252\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9edcd1fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9edcd1fa",
    "outputId": "c411ac59-8c01-43ed-9f59-49f469f2cde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  131177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1,  181,  390, 1000],\n",
       "       [ 181,  390, 1000,    3],\n",
       "       [ 390, 1000,    3,  298],\n",
       "       [1000,    3,  298,    4],\n",
       "       [   3,  298,    4,  946],\n",
       "       [ 298,    4,  946,   41],\n",
       "       [   4,  946,   41, 1000],\n",
       "       [ 946,   41, 1000,   23],\n",
       "       [  41, 1000,   23,   21],\n",
       "       [1000,   23,   21,    1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "\n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49606b44",
   "metadata": {
    "id": "49606b44"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fe716d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7fe716d",
    "outputId": "30507efd-8e61-4e56-f2ec-4658ea0bbe8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060086c4",
   "metadata": {
    "id": "060086c4"
   },
   "source": [
    "# 4. Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cde1b42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cde1b42",
    "outputId": "2da1a34b-ba93-4c07-c5f4-1a196f9ce857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 3, 10)             72520     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 3, 500)            1022000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 500)               2002000   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              501000    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7252)              7259252   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10856772 (41.42 MB)\n",
      "Trainable params: 10856772 (41.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(500, return_sequences=True))\n",
    "model.add(LSTM(500))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a27d6f",
   "metadata": {
    "id": "34a27d6f"
   },
   "source": [
    "# 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8206813a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8206813a",
    "outputId": "270a6fcb-1c81-42cd-9676-ef73397febfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2050/2050 [==============================] - 36s 14ms/step - loss: 6.2696 - accuracy: 0.0554\n",
      "Epoch 2/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 5.6521 - accuracy: 0.1021\n",
      "Epoch 3/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 5.2993 - accuracy: 0.1240\n",
      "Epoch 4/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 5.0591 - accuracy: 0.1374\n",
      "Epoch 5/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 4.8418 - accuracy: 0.1489\n",
      "Epoch 6/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 4.6329 - accuracy: 0.1580\n",
      "Epoch 7/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 4.4252 - accuracy: 0.1670\n",
      "Epoch 8/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 4.2162 - accuracy: 0.1760\n",
      "Epoch 9/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 4.0034 - accuracy: 0.1874\n",
      "Epoch 10/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 3.7863 - accuracy: 0.2045\n",
      "Epoch 11/30\n",
      "2050/2050 [==============================] - 25s 12ms/step - loss: 3.5727 - accuracy: 0.2249\n",
      "Epoch 12/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 3.3659 - accuracy: 0.2500\n",
      "Epoch 13/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 3.1680 - accuracy: 0.2764\n",
      "Epoch 14/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 2.9728 - accuracy: 0.3047\n",
      "Epoch 15/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 2.7905 - accuracy: 0.3351\n",
      "Epoch 16/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 2.6112 - accuracy: 0.3646\n",
      "Epoch 17/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 2.4426 - accuracy: 0.3958\n",
      "Epoch 18/30\n",
      "2050/2050 [==============================] - 24s 11ms/step - loss: 2.2735 - accuracy: 0.4274\n",
      "Epoch 19/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 2.1166 - accuracy: 0.4592\n",
      "Epoch 20/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.9654 - accuracy: 0.4903\n",
      "Epoch 21/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.8191 - accuracy: 0.5234\n",
      "Epoch 22/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.6851 - accuracy: 0.5545\n",
      "Epoch 23/30\n",
      "2050/2050 [==============================] - 24s 12ms/step - loss: 1.5594 - accuracy: 0.5815\n",
      "Epoch 24/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.4435 - accuracy: 0.6110\n",
      "Epoch 25/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.3348 - accuracy: 0.6373\n",
      "Epoch 26/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.2440 - accuracy: 0.6603\n",
      "Epoch 27/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.1579 - accuracy: 0.6813\n",
      "Epoch 28/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.0833 - accuracy: 0.7006\n",
      "Epoch 29/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 1.0183 - accuracy: 0.7170\n",
      "Epoch 30/30\n",
      "2050/2050 [==============================] - 23s 11ms/step - loss: 0.9601 - accuracy: 0.7320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b684bf1b160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=30, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5413a35a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5413a35a",
    "outputId": "763bc1af-42d7-497e-8150-e41968051f4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"next_word_model.h5\")\n",
    "model.save(\"next_word_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f781ce",
   "metadata": {
    "id": "b1f781ce"
   },
   "source": [
    "# 6. Let’s predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33fec44e",
   "metadata": {
    "id": "33fec44e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_word_model.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = np.array(sequence)\n",
    "    preds = np.argmax(model.predict(sequence))\n",
    "    predicted_word = \"\"\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value == preds:\n",
    "            predicted_word = key\n",
    "            break\n",
    "\n",
    "    print(predicted_word)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d85723d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d85723d",
    "outputId": "54f4d08c-b8e0-479a-fb35-2459698969ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: ebook is for the use of anyone\n",
      "['use', 'of', 'anyone']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "anywhere\n",
      "Enter your line: If you are not\n",
      "['you', 'are', 'not']\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "located\n",
      "Enter your line: you will have\n",
      "['you', 'will', 'have']\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "the\n",
      "Enter your line: country where you\n",
      "['country', 'where', 'you']\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "are\n",
      "Enter your line: 0\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    text = input(\"Enter your line: \")\n",
    "\n",
    "    if text == \"0\":\n",
    "        print(\"Execution completed.....\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\" \")\n",
    "            text = text[-3:]\n",
    "            print(text)\n",
    "\n",
    "            Predict_Next_Words(model, tokenizer, text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \",e)\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
